{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>This site hosts the documentation about the Machine Editor for cOnferences Website (MEOW).</p> <p>MEOW is a web application to create JACoW proceedings.  It is triggered by an Indico plugin called Proceedings Utility Running Remotely (PURR).</p> <p>Yes, MEOW and PURR are just two characteristics of CAT.</p>"},{"location":"api/","title":"API","text":"<p>MEOW offers some APIs:</p>"},{"location":"api/#ping","title":"Ping","text":"<p>This API is responsible for validating the pair <code>endpoint</code> - <code>api_key</code>. It is available at <code>/ping/{api_key}</code>. This API simply searches for the client authentication registered with that <code>api_key</code> in the Redis database, returning it to the client if it exists and raising an error if it does not.</p>"},{"location":"api/#info","title":"Info","text":"<p>This API returns some information to the requesting client about the proceedings. It is available at <code>/info/{event_id}/{api_key}</code>. The API initially checks if the requesting client is authenticated and then returns a dictionary like this:</p> <pre><code>{\n  \"event_id\": \"Conference ID\",\n  \"pdf_cache\": \"true if PDF files are cached\",\n  \"pre_press\": \"true if pre-press proceedings have been generated\",\n  \"final_proceedings\": \"true if final proceedings have been generated\",\n  \"datacite_json\": \"true if the DOI JSON payloads have been generated\",\n  \"proceedings_archive\": \"true if an archive of the proceedings exists\"\n}\n</code></pre>"},{"location":"api/#clear","title":"Clear","text":"<p>This API is responsible for deleting all the folders related to a conference by event_id. It is available at <code>/clear/{event_id}/{api_key}</code>. After checking if the authentication is valid, it starts a task that deletes all the PDF files, the proceedings site, and the DOI JSON payloads.</p>"},{"location":"api/#websocket","title":"Websocket","text":"<p>This API is responsible for starting a task to which we usually refer as event. It is available at <code>/{api_key}/{task_id}</code>. After validating the credentials:</p> <ul> <li>a websocket is opened using the <code>task_id</code> sent by the client</li> <li>the task is started. In particular, <code>create_task</code> creates a thread attached to the connection that listens to the websocket messages and publishes on Redis.</li> <li>when the worker has finished, the websocket is closed.</li> </ul>"},{"location":"framework/","title":"Framework","text":"<p>MEOW consists of two software components:</p> <ul> <li>MEOW webapp, which serves as the Web Application Interface.</li> <li>MEOW worker, which is the computational component.</li> </ul> <p>Communication between these two components occurs through Redis PUB/SUB channels.</p>"},{"location":"framework/#meow-webapp","title":"MEOW Webapp","text":"<p>The webapp component is responsible for providing a Web API that allows clients to:</p> <ul> <li>Submit new tasks</li> <li>Retrieve information about the progress status of tasks</li> <li>Retrieve the results of a task</li> <li>Obtain information about the assets produced by tasks</li> </ul>"},{"location":"framework/#meow-worker","title":"MEOW Worker","text":"<p>The worker component offers a list of events and is responsible for initiating them, routing information about their progress and results, and reporting errors.</p>"},{"location":"install/","title":"Installation","text":""},{"location":"install/#server-platform","title":"Server platform","text":"<p>The following instructions refer to an Ubuntu 22.04 server and should be adapted with other package manager commands if a different distribution is chosen.</p>"},{"location":"install/#notes-on-tools-and-versions","title":"Notes on tools and versions","text":""},{"location":"install/#python","title":"Python","text":"<p>CAT is written in Python. At the moment of this deployment Indico officially supports Python versions 3.9 - 3.11, so CAT is based on Python 3.11.</p>"},{"location":"install/#redis","title":"Redis","text":"<p>Redis is used as a communication database for all MEOW tasks. It can be installed in various ways. For simplicity, the official Ubuntu Redis packages are used in MEOW.</p>"},{"location":"install/#supervisord","title":"Supervisord","text":"<p>supervisord is used to manage the MEOW \"service\". It takes care of starting the MEOW processes and restart them if needed.</p>"},{"location":"install/#pdf-tools","title":"PDF tools","text":"<p>qpdf, pdftk and  mupdf are used by MEOW to manipulate the PDFs for final proceedings.</p>"},{"location":"install/#installing-system-packages","title":"Installing System Packages","text":"<pre><code>apt install dbus-broker systemd-container\napt install htop vim p7zip unzip zip git tig\napt install build-essential python3.11-venv python3.11-minimal\napt install qpdf pdftk mupdf-tools mupdf\napt install redis-server redis-tools\n</code></pre>"},{"location":"install/#enabling-and-starting-the-redis-service","title":"Enabling and starting the Redis Service","text":"<pre><code>systemctl enable redis\nsystemctl start redis\n</code></pre>"},{"location":"install/#setup-meow-code","title":"Setup MEOW code","text":"<p>First, let's create the CAT user and access its working directory</p> <pre><code>useradd --comment 'CAT system user' --create-home --system --home-dir /opt/cat --shell /usr/bin/bash cat\nsu - cat\n</code></pre>"},{"location":"install/#downloading-sources","title":"Downloading sources","text":"<pre><code>git clone https://github.com/JACoW-org/MEOW.git\n</code></pre>"},{"location":"install/#setting-python-virtual-environment","title":"Setting Python Virtual Environment","text":"<pre><code>python3 -m venv venv\n. ./venv/bin/activate\n./venv/bin/pip install --upgrade pip\n./venv/bin/pip install -r requirements.txt\n</code></pre>"},{"location":"install/#creating-purrmeow-shared-authentication-key","title":"Creating PURR/MEOW Shared Authentication Key","text":"<p>To create a shared authentication key for requests from the PURR client to the MEOW server, you will need an API key. The following command creates an API key for the user <code>purr</code> on the indico installation at <code>indico.jacow.org</code> and stores it in Redis. You can create different keys for different PURR installations, Indico events or even for other software that would use MEOW's APIs.</p> <pre><code>./venv/bin/python -m meow auth -login purr@indico.jacow.org\n</code></pre> <p>The output will look like this:</p> <pre><code>purr indico.jacow.org &lt;api_key&gt;\n</code></pre> <p>Please take note of this <code>&lt;api_key&gt;</code>: it will be needed in the PURR connection to MEOW settings of the Indico event.</p>"},{"location":"install/#other-meow-useful-commands","title":"Other <code>meow</code> useful commands","text":"<p>List keys present in the DB:</p> <pre><code>./venv/bin/python3 -m meow auth -list\n</code></pre> <p>Check a key:</p> <pre><code>./venv/bin/python3 -m meow auth -check &lt;api_key&gt;\npurr indico.jacow.org &lt;timestamp&gt;\n</code></pre> <p>Remove a key from the DB:</p> <pre><code>./venv/bin/python3 -m meow auth -logout &lt;api_key&gt;\n1\n</code></pre>"},{"location":"install/#starting-supervisord","title":"Starting supervisord","text":"<pre><code>./venv/bin/supervisord -n -c ./conf/supervisord.conf\n</code></pre>"},{"location":"install/#test-that-meow-is-up-and-running","title":"Test that MEOW is up and running","text":"<pre><code>curl http://127.0.0.1:8080/api/ping/&lt;api_key&gt;\n{\n  \"method\": \"ping\",\n  \"params\": {\n    \"user\": \"purr\",\n    \"host\": \"indico.jacow.org\",\n    \"date\": \"2023-10-02T13:51:08.737960\"\n  }\n}\n</code></pre>"},{"location":"events/abstractBooklet/","title":"Abstract Booklet","text":""},{"location":"events/abstractBooklet/#list-of-tasks","title":"List of tasks","text":"<p>There are a total of three tasks involved in generating the Abstract Booklet document:</p> <ol> <li>Data collection</li> <li>Abstract booklet data</li> <li>ODT export</li> </ol>"},{"location":"events/abstractBooklet/#data-collection","title":"Data collection","text":"<p>This task collects the data necessary for creating a conference's abstract booklet, including sessions and contributions. The data is retrieved from Indico. It starts by obtaining the list of sessions and then fetches the list of contributions for each session. The task returns the list of sessions and contributions.</p>"},{"location":"events/abstractBooklet/#abstract-booklet-data","title":"Abstract booklet data","text":"<p>This task takes the list of sessions and contributions as input and organizes the data into a dictionary, which is then returned.</p>"},{"location":"events/abstractBooklet/#odt-export","title":"ODT export","text":"<p>This task is responsible for creating the ODT document using the OdfPy library. You can find more information about OdfPy here.</p> <p>The following subtasks are performed to create the document, after the ODT object is created:</p>"},{"location":"events/abstractBooklet/#document-styles","title":"Document Styles","text":"<p>This task is responsible for defining various styles used in the ODT document. These styles include styles for:</p> <ul> <li>Different levels of headings</li> <li>Tables</li> <li>Paragraphs related to authors, speakers, coauthors, descriptions, and funding agencies</li> <li>Footnotes</li> </ul> <p>The task returns a dictionary containing references to each of the defined styles. These styles can be applied to various elements within the ODT document to control their appearance.</p>"},{"location":"events/abstractBooklet/#abstract-booklet-index","title":"Abstract Booklet Index","text":"<p>This task is responsible for creating an index in the form of a dictionary that contains information about the abstract booklet's content. It follows these steps during execution:</p> <ul> <li>For each session, it generates a unique <code>uuid</code> and captures the <code>code</code> and <code>title</code>.</li> <li>For each contribution within a session, it generates another unique <code>uuid</code> and extracts the <code>code</code> and <code>title</code>.</li> </ul> <p>The resulting index is a valuable resource for efficiently accessing and referencing information about sessions and contributions within the abstract booklet.</p>"},{"location":"events/abstractBooklet/#abstract-booklet-table-of-contents","title":"Abstract Booklet Table of Contents","text":"<p>This task is responsible for generating a table of contents (TOC) within the ODT document for an abstract booklet.</p> <p>The task performs the following key actions:</p> <ul> <li>Initializes a TOC structure within the ODT document.</li> <li>Sets up templates and styles for various heading levels (h1, h2, h3).</li> <li>Defines the appearance and structure of TOC entries.</li> <li>Adds the TOC to the document.</li> </ul> <p>The TOC is then returned by the task.</p>"},{"location":"events/abstractBooklet/#abstract-booklet-body","title":"Abstract Booklet Body","text":"<p>This task generates the body of the abstract booklet based on the settings. It customizes the formatting and content of sessions and contributions. The key settings used include:</p> <ul> <li> <p><code>ab_session_h1</code> and <code>ab_contribution_h1</code>: These settings define the format for session and contribution headers, allowing you to insert dynamic information such as session or contribution codes, titles, start times, and end times.</p> </li> <li> <p><code>ab_session_h2</code> and <code>ab_contribution_h2</code>: Similar to the above settings, these allow for alternative header formats, useful for differentiating between session and contribution styles.</p> </li> </ul>"},{"location":"events/abstractBooklet/#custom-fields","title":"Custom Fields","text":"<p>You can include custom fields in the abstract booklet using the <code>custom_fields</code> settings. This setting lets you specify the names of custom fields you want to include in the booklet. The task checks for these fields in the contributions and adds them to the generated content.</p>"},{"location":"events/abstractBooklet/#document-generation","title":"Document generation","text":"<p>The task iterates through the session and contribution data and uses the configured settings to format the headers and content. Here's how it works:</p> <ul> <li>For each session:<ul> <li>It dynamically replaces placeholders in the session header templates (<code>ab_session_h1</code> and <code>ab_session_h2</code>) with actual session data (code, title, start, and end times).</li> <li>It creates bookmarks for easy reference and adds session chair information if available.</li> <li>It adds a blank line for separation.</li> <li>It adds the session's description.</li> </ul> </li> <li>For each contribution within a session:<ul> <li>It dynamically replaces placeholders in the contribution header templates (<code>ab_contribution_h1</code> and <code>ab_contribution_h2</code>) with actual contribution data (code, title, start, and end times).</li> <li>It creates bookmarks for easy reference.</li> <li>It adds speaker, primary author, and coauthor information based on the configured styles.</li> <li>It includes a description of the contribution.</li> <li>If custom fields are specified, it includes them in the contribution content.</li> </ul> </li> </ul> <p>The task then returns the ODT document.</p> <p>In the end, the event serialises the generated ODT document into a file.</p>"},{"location":"events/compress/","title":"Compress Proceedings","text":"<p>This event is responsible for compressing the previously generated proceedings website.</p>"},{"location":"events/compress/#list-of-tasks","title":"List of Tasks","text":"<p>There are a total of two tasks to be executed, both with a lock on the database:</p> <ol> <li>Adapting Proceedings</li> <li>Compress Static Site</li> </ol>"},{"location":"events/compress/#adapting-proceedings","title":"Adapting Proceedings","text":"<p>This task builds and returns a <code>ProceedingsData</code> object that includes all the conference information.</p>"},{"location":"events/compress/#compress-static-site","title":"Compress Static Site","text":"<p>This task compresses the proceedings website into the <code>.7z</code> format. Initially, it checks if a compressed file for that conference already exists, and if so, it deletes it. Then, it launches the following command in a subprocess:</p> <p><code>bin/7zzs a -t7z -m0=Deflate -ms=16m -mmt=4 -bd -mx=1 -- {event_id}.7z {event_id}</code></p> <ul> <li><code>a</code> is for \"add\" and tells the command to create a new archive.</li> <li><code>-t7z</code> tells the command what type of archive to use, in this case, <code>7z</code>.</li> <li><code>-m0=Deflate</code> is the compression method.</li> <li><code>-ms=16m</code> sets the compression dictionary size to 16MB.</li> <li><code>-mmt=4</code> sets the maximum amount of threads to be used for the compression process to 4.</li> <li><code>-bd</code> tells the command not to show a progress bar.</li> <li><code>-mx=1</code> this option sets the compression level to 1, prioritizing compression speed.</li> <li><code>--</code> is a delimiter that separates the command options from the file names.</li> <li><code>{event_id}.7z</code> is the destination of the compressed archive to be created.</li> <li><code>{event_id}</code> is the name of the file or folder to be compressed.</li> </ul> <p>When the execution of the above command is completed, the task finishes.</p>"},{"location":"events/doi/","title":"DOI","text":"<p>MEOW offers some events that are responsible for handling the DOIs of a conference. These services use DataCite APIs.</p> <p>A DOI can be in the following states:</p> <ul> <li>findable</li> <li>registered</li> <li>draft</li> </ul> <p>A DOI in the findable state is published. A DOI in the registered state is a DOI that has been officially registered in the DataCite repository with a permanent and unique identifier. A DOI in the draft state has just been created and is not fully registered, which means its identifier and metadata can still be edited.</p> <p>The operations that are handled are the following:</p> <ul> <li>DOI info fetching: Retrieves the states of all DOIs in the conference.</li> <li>DOI drafting: Submits the metadata of every contribution in the conference to create the DOIs in the draft state.</li> <li>DOI publishing: Attempts to publish the DOIs of the conference, switching them to the findable state.</li> <li>DOI deletion: Attempts to delete the DOIs of the conference, possible only if they are in the draft state.</li> <li>DOI hiding: Attempts to change the state of the DOIs of the conference from findable to registered.</li> </ul> <p>All the previous events follow the typical workflow of an event in MEOW, which involves a list of tasks.</p>"},{"location":"events/doi/#proceeding-object-building","title":"Proceeding Object Building","text":"<p>This set of tasks includes those responsible for building an instance of <code>ProceedingsData</code> object. It is common to all DOI events:</p> <ul> <li>Retrieving data of sessions and materials from Indico.</li> <li>Retrieving data of contributions from Indico.</li> <li>Combining the data into a single <code>ProceedingsData</code> object.</li> </ul>"},{"location":"events/doi/#event-feature-tasks","title":"Event Feature Tasks","text":""},{"location":"events/doi/#doi-info-fetching","title":"DOI Info Fetching","text":"<p>This task retrieves all the DOI payloads from the folder <code>var/run/{event_id}_doi</code>, and for every contribution, it uses the <code>GET https://api.test.datacite.org/dois/{id}</code> API to retrieve the metadata and therefore the state.</p>"},{"location":"events/doi/#doi-drafting","title":"DOI Drafting","text":"<p>This task retrieves all the DOI payloads from the folder <code>var/run/{event_id}_doi</code>, and for every contribution, it uses the <code>PUT https://api.test.datacite.org/dois/{id}</code> API request to submit the metadata, which, in the default case, results in the creation of the DOI in the draft state.</p>"},{"location":"events/doi/#doi-publishing","title":"DOI Publishing","text":"<p>This task works like the DOI Drafting task, but before making the PUT request, the attribute <code>event</code> of the payload is changed to <code>publish</code>.</p>"},{"location":"events/doi/#doi-deletion","title":"DOI Deletion","text":"<p>This task retrieves all the DOI payloads from the folder <code>var/run/{event_id}_doi</code>, and for every contribution, it uses the <code>DELETE https://api.test.datacite.org/dois/{id}</code>. This task works only for DOIs in the draft state.</p>"},{"location":"events/doi/#doi-hiding","title":"DOI Hiding","text":"<p>This task works like the DOI Drafting task, but before making the PUT request, the attribute <code>event</code> of the payload is changed to <code>hide</code>. This task works only to change the state of DOIs in the <code>findable</code> state to <code>registered</code>.</p>"},{"location":"events/intro/","title":"Introduction","text":"<p>In MEOW, there are several events, each serving a specific purpose:</p> <ul> <li>Abstract Booklet</li> <li>PDF Check</li> <li>Proceedings:<ul> <li>Pre-press proceedings</li> <li>Final proceedings</li> <li>Compression </li> </ul> </li> <li>DOI:<ul> <li>DOI info fetching</li> <li>DOI drafting</li> <li>DOI publishing</li> <li>DOI deletion</li> <li>DOI hiding</li> </ul> </li> </ul> <p>Each event is comprised of a sequence of asynchronous tasks aimed at achieving a particular goal. For instance, these tasks can include generating the \"Abstract Booklet\" document or creating the \"Final proceedings.\" Execution proceeds sequentially, with each task completed before moving on to the next.</p>"},{"location":"events/pdfCheck/","title":"PDF check","text":""},{"location":"events/pdfCheck/#list-of-tasks","title":"List of tasks","text":"<p>This event acquires a lock on the data before executing, renewing it after every task is completed.</p> <p>Following is the list of exploited tasks:</p> <ol> <li>Sessions and Materials Collection</li> <li>Contributions Data Collection</li> <li>Proceedings Data Object Creation</li> <li>Download of the Papers</li> <li>Papers Report</li> <li>Papers Validation</li> </ol> <p>In the end, the event will return a list of dictionaries with metadata and errors.</p>"},{"location":"events/pdfCheck/#sessions-and-materials-collection","title":"Sessions and Materials Collection","text":"<p>This task collects sessions and materials related to the conference based on the provided information. In summary, it starts two parallel subtasks:</p> <ul> <li><code>download_sessions</code>: Retrieves sessions related to the event from Indico and appends them to the <code>sessions</code> list.</li> <li><code>download_materials</code>: Retrieves materials associated with the event from Indico and appends them to the <code>materials</code> list.</li> </ul> <p>The two lists are then returned by the task.</p>"},{"location":"events/pdfCheck/#contributions-data-collection","title":"Contributions Data Collection","text":"<p>This task collects contributions and files associated with a conference based on the provided information. In summary, it starts parallel subtasks <code>download_contributions</code> that retrieves the contributions from Indico and appends them to the <code>contributions</code> list. </p> <p>The list is then returned by the task.</p>"},{"location":"events/pdfCheck/#proceedings-data-object-creation","title":"Proceedings Data Object Creation","text":"<p>This task builds a proceeding's object and then returns it. In particular, for each contribution, it updates the flag <code>is_included_in_pdf_check</code> to <code>True</code> if and only if that contribution has the <code>green</code> or <code>yellow</code> state in Indico.</p>"},{"location":"events/pdfCheck/#download-of-the-papers","title":"Download of the Papers","text":"<p>This task is responsible for downloading contribution papers associated with the proceedings data object. Here's an overview of what happens in this function:</p> <ol> <li> <p>Extracting Papers: It extracts a list of <code>FileData</code> objects representing contribution papers whose contributions have the <code>green</code> or <code>yellow</code> state.</p> </li> <li> <p>Download: For each file data, in parallel, a download subtask is started, that retrieves the PDF file and caches it.</p> </li> </ol> <p>Once all files have been downloaded, the task returns a list containing two sublists: the first sublist contains the updated proceedings object, and the second sublist contains the reference to the downloaded PDFs.</p>"},{"location":"events/pdfCheck/#papers-report","title":"Papers Report","text":"<p>This task operates on the previously created proceedings object. Here's an overview of what happens:</p> <ol> <li> <p>Paper Selection: It builds a list of <code>ContributionPaperData</code> objects representing conference papers that meet the <code>green</code> or <code>yellow</code> state condition.</p> </li> <li> <p>Papers Processing: For each paper, a subtask is initiated to perform the following steps:</p> <ul> <li>Data Collection: The subtask collects various information from the PDF, such as text content, page details, and font information.</li> <li>Keyword Extraction: It extracts keywords from the PDF's text content.</li> <li>Data Organization: All the gathered information is structured into an object.</li> </ul> </li> <li> <p>Proceedings Update: The proceedings object is updated by incorporating all the extracted information.</p> </li> </ol>"},{"location":"events/pdfCheck/#papers-validation","title":"Papers Validation","text":"<p>This task validates data based on certain criteria. Here's an overview of what it does:</p> <ol> <li> <p>Initialization: It retrieves page width and height settings from the settings.</p> </li> <li> <p>Data Collection and Validation Loop: For each contribution in the proceedings data:</p> <ul> <li>Metadata is extracted and stored.</li> <li>Error checks are performed:<ul> <li>Page size is checked against the specified PDF page width and height.</li> <li>Font embedding is verified.</li> </ul> </li> <li>If errors are detected, they are appended to the <code>errors</code> list along with contribution details.</li> </ul> </li> <li> <p>Results: The function returns a list containing both the collected metadata and any errors encountered during validation.</p> </li> </ol>"},{"location":"events/proceedings/","title":"Proceedings","text":""},{"location":"events/proceedings/#filtering-function","title":"Filtering Function","text":"<p>An important callback function is defined within the event function: <code>filter_published_contributions</code>. This function is employed to filter contributions, and the filtering process plays a crucial role in distinguishing between the generation of final and pre-press proceedings.</p> <p>Here's how the filtering of contributions differs:</p> <ul> <li>For final proceedings, only contributions in the <code>green</code> state that have received QA approval are included.</li> <li>For pre-press proceedings, only contributions in the <code>green</code> state are included.</li> </ul>"},{"location":"events/proceedings/#list-of-tasks","title":"List of tasks","text":"<p>This event acquires a lock on the data before executing, renewing it after every task is completed.</p> <p>Following is the list of tasks:</p> <ol> <li>Sessions and Materials Collection</li> <li>Contributions Collection</li> <li>Proceedings Adapting</li> <li>Static Site Cleaning</li> <li>Conference Materials Download</li> <li>Contribution Papers Download</li> <li>Contribution Slides Download</li> <li>Read Papers Metadata</li> <li>Validate Proceedings Data</li> <li>Generate Contributions References</li> <li>Generate DOIs</li> <li>Build DOI payloads</li> <li>Manage duplicates</li> <li>Write Papers Metadata</li> <li>Generate Contributions Groups</li> <li>Concatenate Contributions Papers</li> <li>Generate Site Pages</li> <li>Copy Event PDFs</li> <li>Generate Proceedings</li> <li>Link Static Site</li> </ol>"},{"location":"events/proceedings/#sessions-and-materials-collection","title":"Sessions and Materials Collection","text":"<p>This task is responsible for retrieving session and material data from Indico and providing it to the main task. Since a conference may contain more materials than necessary for the proceedings generation, the task filters the materials obtained from Indico, retaining only those specified in PURR settings.</p>"},{"location":"events/proceedings/#contributions-collection","title":"Contributions Collection","text":"<p>This task collects contributions and files associated with a conference based on the provided information. In summary, it starts parallel subtasks <code>download_contributions</code> that retrieves the contributions from Indico and appends them to the <code>contributions</code> list. </p> <p>The list is then returned by the task.</p>"},{"location":"events/proceedings/#proceedings-adapting","title":"Proceedings Adapting","text":"<p>In this task, a proceedings object is constructed and subsequently returned. The primary objective of this task is to consolidate all the data into a single data structure and deserialize the data from the preceding tasks into the appropriate structures.</p> <p>Additionally, it updates the following flags for each contribution:</p> <ul> <li><code>is_included_in_proceedings</code> is set to <code>True</code> when the contribution is in the <code>green</code> state and has received <code>qa_approval</code>.</li> <li><code>is_included_in_prepress</code> is set to <code>True</code> when the contribution is in the <code>green</code> state.</li> </ul> <p>These flags serve as one of the primary distinctions between final and pre-press proceedings. Their values are utilized throughout the generation process to determine whether or not to include a contribution.</p>"},{"location":"events/proceedings/#static-site-cleaning","title":"Static Site Cleaning","text":"<p>This task is responsible for cleaning up and removing temporary files and directories associated with the static site generation process of a previous run for this conference.</p>"},{"location":"events/proceedings/#conference-materials-download","title":"Conference Materials Download","text":"<p>This task retrieves files using the list of materials from the proceedings object and stores them under a temporary folder.</p>"},{"location":"events/proceedings/#contribution-papers-download","title":"Contribution Papers Download","text":"<p>This task is responsible for downloading contribution papers associated with the proceedings data object. Here's an overview of what happens in this function:</p> <ol> <li> <p>Extracting Papers: After filtering the contributions using the filtering function, this task extracts a list of <code>FileData</code> objects that reference PDF papers in Indico.</p> </li> <li> <p>Download: For each file data, a download subtask is initiated in parallel to retrieve the PDF file and cache it.</p> </li> </ol> <p>Once all files have been downloaded, the task returns a list containing two sublists: the first sublist contains the updated proceedings object, and the second sublist contains references to the downloaded PDFs.</p>"},{"location":"events/proceedings/#contribution-slides-download","title":"Contribution Slides Download","text":"<p>This task is executed only for the generation of the final proceedings. Initially, the task compiles a list of files to be retrieved by filtering contributions that have slides in their latest revision. Then, all the files are retrieved from Indico using parallel subtasks and stored under a temporary folder.</p>"},{"location":"events/proceedings/#read-papers-metadata","title":"Read Papers Metadata","text":"<p>This task initially retrieves the list of papers for which metadata needs to be read.</p> <p>For each PDF file, the task fetches metadata, including keywords, fonts, page width, and height, along with the total number of pages.</p> <p>All the gathered information is then collected and returned.</p> <p>Subsequently, another subtask updates the proceedings object by iterating through the contributions and updating the following properties:</p> <ul> <li>Paper size (in bytes)</li> <li>List of keywords</li> <li>Page number (based on the total number of contributions)</li> <li>Fonts</li> <li>Page width and height</li> </ul>"},{"location":"events/proceedings/#validate-proceedings-data","title":"Validate Proceedings Data","text":"<p>This task is responsible for validating the data that has been collected thus far and returning errors to the client. To accomplish this, the metadata of each contribution is examined against the following criteria:</p> <ul> <li>The page count is not null.</li> <li>The paper's width and height are correct.</li> <li>The paper's fonts are embedded.</li> </ul> <p>While errors do not obstruct the execution, they will be logged for the client's reference.</p>"},{"location":"events/proceedings/#generate-contributions-references","title":"Generate Contributions References","text":"<p>This task is responsible for generating references for the contributions. The following formats are used for generating references:</p> <ul> <li>BibTeX</li> <li>LaTeX</li> <li>Word</li> <li>RIS</li> <li>EndNote</li> </ul> <p>The task builds a XML string for each contribution, that includes all the metadata that is needed to generate all the references. Subsequently, for each format, there is a dedicated XSLT file that is capable of building the reference string in the appropriate format. You can find them here. Finally, the proceeding object is updated by incorporating the generated contribution references.</p>"},{"location":"events/proceedings/#about-xslt","title":"About XSLT","text":"<p>XSLT is an acronym for Extensible Stylesheet Language Transformations. It is a language primarily used for transforming and rendering XML documents. XSLT is designed to convert XML data into various formats, such as HTML, plain text, or even other XML documents.</p> <p>Here is a list of key points about XSLT:</p> <ul> <li> <p>Transformation: XSLT is used to transform XML documents into other XML documents or different formats, defining rules for how the transformation should occur.</p> </li> <li> <p>Templates: XSLT operates by applying templates, which are patterns that match elements or nodes in the source XML document. Templates specify how matched parts should be transformed in the output.</p> </li> <li> <p>XPath: XPath is used within XSLT to navigate and select nodes within XML documents, allowing precise control over which parts should be transformed and how.</p> </li> <li> <p>Output Formatting: XSLT lets you control the formatting of the output document, including specifying the structure, attributes, and data presentation.</p> </li> <li> <p>Recursive Processing: XSLT supports recursive processing, enabling the same transformation logic to be applied to different sections of hierarchical XML data.</p> </li> <li> <p>Platform-Independent: XSLT is platform-independent and widely supported by various programming languages and tools.</p> </li> <li> <p>Common Use Cases: XSLT is commonly used for converting XML data into HTML for web display, generating reports, and transforming XML data for integration with other systems.</p> </li> </ul> <p>XSLT is a valuable tool for transforming XML data into different formats, making it suitable for various web development, document processing, and data integration tasks.</p>"},{"location":"events/proceedings/#generate-dois","title":"Generate DOIs","text":"<p>This task generates a <code>ContributionDOI</code> object for each contribution in the conference. <code>ContributionDOI</code> includes all the metadata necessary to create the DOI page for each contribution.</p> <p>Additionally, a dictionary containing all the data for the conference's DOI is also generated.</p> <p>Finally, the proceeding object is updated by incorporating the generated DOIs.</p>"},{"location":"events/proceedings/#build-doi-payloads","title":"Build DOI Payloads","text":"<p>This task is executed only during the generation of the final proceedings and is responsible for creating JSON payloads to use the datacite.org API for generating DOIs for each contribution.</p> <p>To generate the JSON string, the <code>ContributionDOI</code> class includes an <code>as_json()</code> function that constructs a dictionary with all the required metadata and then converts it into a string.</p> <p>The resulting JSON strings are then individually written to JSON files for future use.</p> <p>The same process is done also for conference's DOI payload.</p>"},{"location":"events/proceedings/#manage-duplicates","title":"Manage Duplicates","text":"<p>This task deals with contributions that have been marked as <code>duplicate_of</code>. For these contributions, a <code>DuplicateContributionData</code> object is created, which contains information about the contribution to which the current one is a duplicate. This information includes the DOI URL and the dates of reception, revision, acceptance, and issuance.</p>"},{"location":"events/proceedings/#write-papers-metadata","title":"Write Papers Metadata","text":"<p>This task manages the writing of metadata into the PDF files of the papers. For each file, a writing task is initiated, with parallel execution.</p> <p>Additionally, the task is responsible for generating the footers and headers for the pages of each paper.</p>"},{"location":"events/proceedings/#generate-contributions-groups","title":"Generate Contributions Groups","text":"<p>This task is responsible for grouping contributions based on various attributes, including:</p> <ul> <li>Session</li> <li>Classification</li> <li>Author</li> <li>Institute</li> <li>DOI per Institute</li> <li>Keyword</li> </ul> <p>These groups serve as indexing modes for contributions on the final website.</p>"},{"location":"events/proceedings/#concatenate-contributions-papers","title":"Concatenate Contributions Papers","text":"<p>This task is responsible for concatenating the papers to create the following volumes:</p> <ul> <li>\"Proceedings at a Glance,\" which includes the first pages of all the papers. Each page also serves as a hypertextual link to the respective paper.</li> <li>\"Proceedings Volume,\" which includes all the papers of the conference.</li> </ul> <p>Both volumes may have a cover if added through the materials, and a table of contents is generated based on the settings.</p> <p>To expedite the process, papers are divided into chunks (based on their page numbers) and concatenated in parallel.</p> <p>The tool used to perform this task is PDFtk.</p>"},{"location":"events/proceedings/#generate-site-pages","title":"Generate Site Pages","text":"<p>This task operates in three steps:</p> <ol> <li> <p>An instance of <code>HugoFinalProceedingsPlugin</code> is created. Starting from the proceeding object, it initializes all the variables needed to generate the static web pages. Additionally, it also initializes the <code>Path</code> variables pointing to the folders where these pages will be stored.</p> </li> <li> <p>The <code>run_prepare</code> function actively creates the folders using the <code>Path</code> variables initialized previously. It then initializes a <code>JinjaTemplateRenderer</code> object that renders the <code>config.toml</code> and <code>index.html</code> files from their respective Jinja templates.</p> </li> <li> <p>The <code>run_build</code> function generates all other website pages, executing them in parallel to expedite the process.</p> </li> </ol> <p>Upon completion of this phase, all website pages will be found under the <code>var/run/{event_id}_src</code> folder.</p>"},{"location":"events/proceedings/#copy-event-pdfs","title":"Copy Event PDFs","text":"<p>This task is responsible for copying the PDF files retrieved in one of the previous tasks to the folder <code>var/run/{event_id}_src/static/pdf</code>. These assets are also necessary to build the final website. The subtasks are as follows:</p> <ul> <li><code>copy_event_materials</code> for the materials (excluding the covers).</li> <li><code>copy_contribution_papers</code> for the PDFs of the papers.</li> <li><code>copy_contribution_slides</code> for the slides of the contributions, but only for the final proceedings event.</li> </ul>"},{"location":"events/proceedings/#generate-proceedings","title":"Generate Proceedings","text":"<p>Now that all the folders and files are in place, this task can run the command:</p> <p><code>bin/hugo --source var/run/{event_id}_src --destination out</code></p> <ul> <li><code>--source</code> instructs Hugo to use the specified folder as the source of the website, where all the generated files are stored.</li> <li><code>--destination</code> specifies where Hugo should create the website, which, in this case, is the <code>out</code> folder.</li> </ul>"},{"location":"events/proceedings/#link-static-site","title":"Link Static Site","text":"<p>This task is responsible for updating the preview of the proceedings website. It achieves this by moving the newly generated website to the <code>site_preview_path</code>. Once the move is complete, it deletes the website from the old folder to free up space.</p>"},{"location":"tools/hugo/","title":"Hugo","text":"<p>Hugo is a popular open-source static website generator written in Go. It's designed to create high-performance static websites with ease and speed. Some of the features and key points of Hugo include:</p> <ul> <li> <p>High Speed: Hugo is renowned for its incredibly fast generation. It utilizes parallel processing and a range of internal optimizations to generate static websites in a matter of milliseconds.</p> </li> <li> <p>Customizable Themes: Hugo supports customizable themes, allowing you to control the appearance and layout of your site.</p> </li> </ul> <p>MEOW includes Hugo's binaries and executes them in a subprocess.</p>"},{"location":"tools/jinja/","title":"Jinja","text":"<p>Jinja is a popular templating engine for Python, inspired by Django's templating system. It allows you to embed dynamic content and logic into templates, making it easier to generate dynamic HTML, XML, JSON or any other text-based format.</p> <p>Jinja is extensively used in MEOW in order to generate the proceedings' site pages or the contributions' references.</p> <p>Here are some key features and concepts related to Jinja's template rendering:</p> <ul> <li>Template Tags: Jinja uses special tags enclosed in double curly braces <code>{}</code> to render dynamic content or expressions. For example, <code>{{ variable_name }}</code> is used to insert the value of a variable into the template.</li> <li>Variables: You can define variables in your templates and then reference them using the <code>{{ variable_name }}</code> syntax. These variables can hold dynamic data like strings, numbers, or even complex objects.</li> <li>Control Structures: Jinja supports control structures like if, for, and while. You can use <code>{% if condition %}...{% endif %}</code> to conditionally render content or <code>{% for item in items %}...{% endfor %}</code> to iterate over a list or dictionary.</li> <li>Filters: Filters in Jinja are used to modify the output of variables or expressions. For example, you can use <code>{{ variable_name|filter_name }}</code> to apply a filter to a variable. Filters can be used for tasks like formatting dates, converting text to uppercase, or escaping HTML.</li> <li>Template Inheritance: Jinja supports template inheritance, allowing you to create a base template with a structure common to multiple pages and then extend it in child templates. This DRY (Don't Repeat Yourself) approach simplifies template management.</li> <li>Macro Definitions: Macros are reusable code blocks defined in Jinja templates. They can accept parameters and are similar to functions or methods in programming. Macros are useful for avoiding code duplication.</li> <li>Custom Extensions: Jinja allows you to create custom extensions or filters to extend its functionality. This can be particularly useful when you need to perform custom logic within your templates.</li> <li>Escaping: Jinja automatically escapes content to prevent cross-site scripting (XSS) attacks. However, you can explicitly mark content as safe to disable escaping when you trust the source.</li> <li>Comments: Jinja templates support comments using the <code>{# ... #}</code> syntax. Comments are not displayed in the rendered output.</li> <li>Error Handling: Jinja provides detailed error messages to help you diagnose and fix issues in your templates. This makes debugging easier.</li> </ul> <p>In MEOW, there is a <code>jinja</code> folder that includes all the templates. In order to generate the text from a template, we define a custom renderer, that includes a custom render function, that wraps the original Jinja render and builds the dictionary of variables from a custom object.</p>"},{"location":"tools/keywords/","title":"Keywords Extraction","text":"<p>The algorithm for extracting keywords follows a structured pipeline with multiple phases.</p>"},{"location":"tools/keywords/#default-keywords-stems-tree","title":"Default Keywords Stems Tree","text":"<p>The algorithm commences with a predefined list of keywords that serve as fundamental features for the extraction process. This approach aims to create an efficient keyword extractor without relying on natural language processors.</p> <p>The list of default keywords can be found in <code>keywords.py</code>. It also encompasses combined keywords, which present a notable challenge. Certain words may change their meaning based on the context in which they are used.</p> <p>During this phase, starting from the default keyword list, the algorithm constructs a dictionary. In this dictionary, the keys represent stems derived from the first tokens of the keywords, and the corresponding values are lists of keywords sharing the same stem. This grouping facilitates the efficient matching of similar keywords. This dictionary is referred to as <code>keywords_stems_tree</code>.</p>"},{"location":"tools/keywords/#keyword-extraction","title":"Keyword Extraction","text":"<p>In this phase, the algorithm initially tokenizes the text into a list of words. It subsequently removes all stopwords based on the Snowball stemmer. The algorithm then iterates through each token, obtaining its stem. If the stem is found in the <code>keywords_stems_tree</code>, the algorithm enters a nested loop to check if the token sequence matches any of the keywords associated with that stem. If a match is found, it increments the count for that specific keyword.</p>"},{"location":"tools/keywords/#keyword-ranking","title":"Keyword Ranking","text":"<p>The keyword-count pairs are sorted in descending order, primarily based on the count. The algorithm ultimately returns the top keywords while ensuring that keywords with the same count as the fifth-ranked keyword are included.</p> <p>This structured approach allows the algorithm to efficiently identify and prioritize keywords within a given text.</p>"}]}